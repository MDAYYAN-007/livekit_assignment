./cli_chat.py
-----
from core.engine import PitchEngine

engine = PitchEngine()

print("\nüöÄ PitchSense CLI\n")
print("Type 'exit' to quit.\n")

while True:
    user_input = input("You: ")

    if user_input.lower() in ["exit", "quit"]:
        print("Goodbye.")
        break

    result = engine.evaluate_pitch(user_input)

    print("\nPitchSense:")
    print(result["mentor_response"])
    print("\n" + "-"*60 + "\n")./tools/tools.py
-----
from langchain.tools import tool
from tools.pdf_generator import generate_pitch_pdf


@tool
def export_pitch_pdf(content: str) -> str:
    """
    Generates a downloadable PDF using the provided content.
    The content should already be structured and formatted by the LLM.
    """

    data = {
        "PitchSense Report": content
    }

    filename = generate_pitch_pdf(data)

    return f"PDF generated successfully: {filename}"./tools/pdf_generator.py
-----
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.units import inch
from reportlab.lib.styles import getSampleStyleSheet

def generate_pitch_pdf(data: dict, filename: str = "pitch_report.pdf"):

    doc = SimpleDocTemplate(filename)
    elements = []
    styles = getSampleStyleSheet()

    elements.append(Paragraph("PitchSense Report", styles["Heading1"]))
    elements.append(Spacer(1, 0.3 * inch))

    for key, value in data.items():
        elements.append(Paragraph(f"<b>{key}</b>", styles["Heading2"]))
        elements.append(Spacer(1, 0.1 * inch))

        if isinstance(value, list):
            for item in value:
                elements.append(Paragraph(f"- {item}", styles["Normal"]))
        else:
            elements.append(Paragraph(str(value), styles["Normal"]))

        elements.append(Spacer(1, 0.2 * inch))

    doc.build(elements)

    return filename./memory/session_memory.py
-----
# memory/session_memory.py

import json
import os

MEMORY_FILE = "memory/session.json"


def load_memory():
    if not os.path.exists(MEMORY_FILE):
        return {
            "founder_name": "",
            "startup_idea": "",
            "target_market": "",
            "business_model": "",
            "key_strengths": [],
            "known_gaps": [],
            "refinement_progress": "",
            "conversation_stage": "discovery",
            "evaluation_ready": False

        }

    with open(MEMORY_FILE, "r") as f:
        return json.load(f)


def save_memory(memory):
    with open(MEMORY_FILE, "w") as f:
        json.dump(memory, f, indent=2)./retrieval/build_index.py
-----
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from vectorstore import get_embedding_model, save_vectorstore

PDF_FOLDER = "data/pitch_docs"

def load_documents():
    docs = []
    for file in os.listdir(PDF_FOLDER):
        if file.endswith(".pdf"):
            loader = PyPDFLoader(os.path.join(PDF_FOLDER, file))
            loaded_docs = loader.load()
            for doc in loaded_docs:
                doc.metadata["source"] = file
            docs.extend(loaded_docs)
    return docs

def chunk_documents(documents):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=100,
    )
    return splitter.split_documents(documents)

def build_index():
    print("Loading PDFs...")
    docs = load_documents()

    print("Chunking documents...")
    chunks = chunk_documents(docs)

    print("Creating embeddings...")
    embeddings = get_embedding_model()

    print("Building FAISS index...")
    vectorstore = FAISS.from_documents(chunks, embeddings)

    print("Saving index...")
    save_vectorstore(vectorstore)

    print("Index built successfully.")

if __name__ == "__main__":
    build_index()./retrieval/vectorstore.py
-----
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
INDEX_PATH = "retrieval/faiss_index"

def get_embedding_model():
    return HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

def load_vectorstore():
    embeddings = get_embedding_model()
    return FAISS.load_local(
        INDEX_PATH,
        embeddings,
        allow_dangerous_deserialization=True,
    )


def save_vectorstore(vectorstore):
    vectorstore.save_local(INDEX_PATH)./agent.py
-----
from livekit.agents import (
    AgentSession,
    Agent,
    ConversationItemAddedEvent,
    JobContext,
    WorkerOptions,
    cli,
    AutoSubscribe,
)

from core.engine import PitchEngine


engine = PitchEngine()


class PitchSenseAgent(Agent):
    def __init__(self):
        super().__init__(
            instructions="You are PitchSense, an AI startup pitch mentor."
        )

    async def on_message(self, message, context):
        result = engine.evaluate_pitch(message)
        return result["mentor_response"]
    
async def entrypoint(ctx: JobContext):

    await ctx.connect(auto_subscribe=AutoSubscribe.NONE)

    session = AgentSession()

    @session.on("conversation_item_added")
    async def handle_message(ev: ConversationItemAddedEvent):

        if not hasattr(ev, "item"):
            return

        role = getattr(ev.item, "role", "")
        text = getattr(ev.item, "text_content", "")

        if role == "user" and text:
            result = engine.evaluate_pitch(text)
            await session.say(result["mentor_response"])

    await session.start(
        agent=Agent(instructions="You are PitchSense."),
        room=ctx.room,
    )

if __name__ == "__main__":
    cli.run_app(
        WorkerOptions(
            entrypoint_fnc=entrypoint,
            agent_name="pitchsense-agent"
        )
    )./core/prompt.py
-----
from langchain_core.prompts import PromptTemplate

mentor_prompt = PromptTemplate.from_template(
"""
SYSTEM ROLE:
You are PitchSense ‚Äî an elite startup accelerator mentor.
You are analytical, structured, and strategic.
You are NOT a grading bot.
You should not answer anything too much away from the domain "startup pitch mentor".
If something is asked out of the domain try to center it towards domain.

CURRENT SESSION MEMORY:
{memory}

RETRIEVED FRAMEWORK:
{context}

FOUNDER MESSAGE:
{input}

CONVERSATION STAGE:
Possible stages:
- discovery
- clarification
- validation
- strategy
- evaluation_ready

STAGE RULES:
- If idea unclear ‚Üí discovery
- If idea clear but positioning vague ‚Üí clarification
- If model unclear ‚Üí validation
- If business model & differentiation clear ‚Üí strategy
- If no major gaps remain ‚Üí evaluation_ready

EVALUATION RULE:
Set evaluation_ready = true ONLY IF:
- Problem clearly defined
- Target market specific
- Business model explained
- Differentiation articulated
- No major known_gaps remain

If evaluation_ready becomes true:
- You MAY offer a structured evaluation.
- But you should still provide strategic insights, improvements, and refinement suggestions.
- Do NOT repeatedly ask for evaluation if already offered.
- Continue acting like a real mentor.

MEMORY RULES:
- Preserve founder_name and startup_idea.
- refinement_progress ‚â§ 400 characters.
- Rewrite summary instead of appending.
- Remove resolved gaps.

PDF RULE:
You may call export_pitch_pdf ONLY when the founder explicitly requests a downloadable PDF.
If the request is ambiguous, ask for clarification.
Do NOT generate a PDF automatically after evaluation.

If the founder asks to download, export, or generate a PDF,
you MUST call export_pitch_pdf.

Prepare the complete formatted report content
and pass it as the "content" argument to the tool.

OUTPUT STRICTLY JSON:

{{
  "mentor_response": "...",
  "memory_update": {{
      "founder_name": "...",
      "startup_idea": "...",
      "target_market": "...",
      "business_model": "...",
      "key_strengths": ["..."],
      "known_gaps": ["..."],
      "refinement_progress": "...",
      "conversation_stage": "...",
      "evaluation_ready": true/false
  }}
}}
"""
)./core/engine.py
-----
import json
from langchain_groq import ChatGroq
from retrieval.vectorstore import load_vectorstore
from memory.session_memory import load_memory, save_memory
from core.prompt import mentor_prompt
from tools.tools import export_pitch_pdf


class PitchEngine:

    def __init__(self):
        base_llm = ChatGroq(
            model="llama-3.3-70b-versatile",
            temperature=0.4,
        )

        self.llm = base_llm.bind_tools([export_pitch_pdf])

        self.vectorstore = None
        self.retriever = None

    def evaluate_pitch(self, user_input):

        if self.vectorstore is None:
            self.vectorstore = load_vectorstore()
            self.retriever = self.vectorstore.as_retriever(search_kwargs={"k": 4})

        memory = load_memory()

        docs = self.retriever.invoke(user_input)
        context_text = "\n\n".join([d.page_content for d in docs])

        prompt = mentor_prompt.format(
            memory=json.dumps(memory, indent=2),
            context=context_text,
            input=user_input
        )

        result = self.llm.invoke(prompt)

        # TOOL CALL
        if hasattr(result, "tool_calls") and result.tool_calls:
            tool_call = result.tool_calls[0]
            tool_result = export_pitch_pdf.invoke(tool_call["args"])
            return {"mentor_response": tool_result}

        # NORMAL JSON FLOW
        parsed = json.loads(result.content)

        new_memory = parsed.get("memory_update", {})

        updated_memory = memory.copy()
        for key in updated_memory.keys():
            if key in new_memory and new_memory[key]:
                updated_memory[key] = new_memory[key]

        save_memory(updated_memory)

        return parsed./generate_token.py
-----
from livekit import api
import jwt

# üîê Replace with your LiveKit Cloud credentials
api_key = "APIM3u3ABkCYfNk"
api_secret = "ROk68aI3vSp5EZZLd9ZvjrLWi40IUbghRcvLItrxhbE"

room_name = "99900yyanannahdiijjinhking"  # Must match the room name used in AgentSession

# Build token
token = (
    api.AccessToken(api_key, api_secret)
    .with_identity("user1")
    .with_name("User One")
    .with_grants(
        api.VideoGrants(
            room_join=True,
            room=room_name,
        )
    )
    .with_room_config(
        api.RoomConfiguration(
            agents=[
                api.RoomAgentDispatch(
                    agent_name="pitchsense-agent"
                )
            ]
        )
    )
)

jwt_token = token.to_jwt()

print("\n========== LIVEKIT TOKEN ==========\n")
print(jwt_token)
print("\n===================================\n")

decoded = jwt.decode(jwt_token, options={"verify_signature": False})
print("Decoded payload:\n")
print(decoded)